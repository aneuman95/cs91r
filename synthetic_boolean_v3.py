# -*- coding: utf-8 -*-
"""Synthetic Adversarial Experiment.ipynb
Automatically generated by Colaboratory.
Original file is located at
    https://colab.research.google.com/drive/1PVdSWqUjKxD2VzOpdh1pXbkVMKm2avnm
"""

# http://pytorch.org/
from os.path import exists
from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag
#platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())
#cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\.\([0-9]*\)\.\([0-9]*\)$/cu\1\2/'
#accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'

#!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision
import torch

import sys, argparse, re, inspect
import numpy as np
from torch.autograd import Function #, Variable
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import random
import string
from statistics import mean 
import json 


seed = 1
random.seed(seed)
np.random.seed(seed)
torch.manual_seed(seed)

CONTRADICTS = 0
NEUTRAL = 1
ENTAILS = 2

word_to_idx = {'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': 7, 'i': 8, 'j': 9,
'k':10, 'l': 11, 'm': 12, 'n': 13, 'o': 14, 'p': 15, 'q': 16, 'r': 17, 's': 18, 't': 19, 'u': 20,
'v': 21, 'w': 22, 'x': 23, "y": 24, 'z': 25, '~': 26, '(': 27, ')': 28, '&': 29, '|': 30, '^': 31, '>': 32} 

def make_tensor(st):
    
    return torch.tensor([word_to_idx[c] for c in list(st)], dtype=torch.long)

def make_data(json_file):

    #load JSON file with entailment data 
    with open(json_file) as f:
        data = json.load(f)

    #convert all labels to integers 
    for i in range(len(data['lbls'])):
        data['lbls'][i] = int(data['lbls'][i])

    return data

# models

class Encoder(nn.Module):
    def __init__(self, embedding, embedding_dim, hidden_dim, directions):
        super(Encoder, self).__init__()      
        self.embedding = embedding
        self.embedding_dim = 10
        self.hidden_dim = 10
        self.directions = 1

        #self.lstm = nn.LSTM(embedding_dim=self.embedding_dim, hidden_dim=self.hidden_dim, birectional=(self.directions == 2))
        self.lstm = nn.LSTM(10, 10)
        (self.h0,self.c0) = self.init_hidden(self.hidden_dim)

    def init_hidden(self, dim):
        # The axes semantics are (num_layers, minibatch_size, hidden_dim)
        # Yonatan, is self.directions the correct input below, or should it be something else for each? 
        return (torch.zeros(self.directions, 1, dim), torch.zeros(self.directions, 1, dim))
        
    def forward(self, x):
        emb = self.embedding(x)
        #Yonatan, do we need any of the below 3 lines (they are from the original Encoder)? 
        emb = torch.squeeze(emb, 0)
        emb = torch.sum(emb, 0)
        emb = torch.reshape(emb, (1, emb.nelement())) 

        #Yonatan, could we simply return hn[-1] to get the last hidden elements?   
        lstm_out, (hn,cn) = self.lstm(emb.view(len(x), 1, -1), (self.h0,self.c0))
        return hn[-1]

class GradReverse(Function):

    def __init__(self, lambd=1.0):
        self.lambd = lambd

    def forward(self, x):
        return x.view_as(x)

    def backward(self, grad_output):
        return (grad_output * -self.lambd)

def grad_reverse(x, lambd=1.0):
    return GradReverse(lambd)(x)



"""
Module for adversarial encoding 
"""
class HypothNet(nn.Module):
    def __init__(self, embedding_dim, num_classes, adv_hyp_encoder_lambda, encoder, hypoth_length=1, op='cat'):
        super(HypothNet, self).__init__()
        
        self.op = op
        if self.op == 'cat':
            self.embedding_dim = embedding_dim * hypoth_length
        else:
            self.embedding_dim = embedding_dim 
        print('HypothNet op:', self.op)
            
        self.num_classes = num_classes
        self.adv_hyp_encoder_lambda = adv_hyp_encoder_lambda
        
        self.classifier = nn.Sequential(
            nn.Linear(self.embedding_dim, self.embedding_dim),
            nn.ReLU(),
            nn.Linear(self.embedding_dim, self.num_classes))        
        self.encoder = encoder

        
    def forward(self, h):
        v = self.encoder(h)
        # reverse gradients going into the encoder
        v = grad_reverse(v, self.adv_hyp_encoder_lambda)

        output = self.classifier(v)
        return output


class NLINet(nn.Module):
    def __init__(self, embedding_dim, num_classes, adv_hyp_encoder_lambda, encoder_p, encoder_h, hypoth_length=1, op='cat'):
        super(NLINet, self).__init__()

        self.op = op
        if self.op == 'cat':
            self.embedding_dim = embedding_dim + embedding_dim * hypoth_length
        else:
            self.embedding_dim = embedding_dim * 2       
        print('NLINet op:', self.op)
        self.num_classes = num_classes
        self.adv_hyp_encoder_lambda = adv_hyp_encoder_lambda
        
        self.classifier = nn.Sequential(
            nn.Linear(self.embedding_dim, self.embedding_dim),
            nn.ReLU(),
            nn.Linear(self.embedding_dim, self.num_classes))
        
        self.encoder_p = encoder_p
        self.encoder_h = encoder_h

        
    def forward(self, p, h, random_p=False):
        #print(p)
        #print(h)        
        u = self.encoder_p(p)
        v = self.encoder_h(h)
        #print(u)
        #print(v)
        if random_p:
            # block gradients from back-propagating to the premise encoder 
            u = grad_reverse(u, 0.0)
            # reverse gradients when back-propagating to the hypothesis encoder
            v = grad_reverse(v, self.adv_hyp_encoder_lambda)
        

        #features = torch.cat((u, v, torch.abs(u-v), u*v), 1)
        features = torch.cat((u, v), 1)
        #print('features:', features)
        output = self.classifier(features)
        #print('output:', output)
        return output

def trainepoch(epoch, train, optimizer_nli, optimizer_hypoth, params, word_vec, nli_net, hypoth_net, \
                             loss_fn_nli, loss_fn_hypoth, adv_lambda, adv_hyp_encoder_lambda):
    print('\nTRAINING : Epoch ' + str(epoch))
    nli_net.train()
    hypoth_net.train()
    all_costs_nli, all_costs_hypoth = [], []
    logs = []
    #words_count = 0

    correct_nli, correct_hypoth = 0., 0.
    # shuffle the data
    permutation = np.random.permutation(len(train['hypoths']))

    hypoths, premises, targets = [], [], [] 
    for i in permutation:
        hypoths.append(train['hypoths'][i])
        premises.append(train['premises'][i])
        targets.append(train['lbls'][i])

    optimizer_nli.param_groups[0]['lr'] = optimizer_nli.param_groups[0]['lr'] * params.decay if epoch>1\
            and 'sgd' in params.optimizer else optimizer_nli.param_groups[0]['lr']
    print('Learning rate (nli) : {0}'.format(optimizer_nli.param_groups[0]['lr']))
    optimizer_hypoth.param_groups[0]['lr'] = optimizer_hypoth.param_groups[0]['lr'] * params.decay if epoch>1\
            and 'sgd' in params.optimizer else optimizer_hypoth.param_groups[0]['lr']
    print('Learning rate (hypoth) : {0}'.format(optimizer_hypoth.param_groups[0]['lr']))

    #trained_sents = 0

    for stidx in range(0, len(hypoths)):
        
        h = make_tensor(hypoths[stidx])
        target = torch.tensor(targets[stidx], dtype=torch.long)

        if np.random.random() > params.random_premise_frac:
            random_premise = False
            p = make_tensor(premises[stidx])
        else:
            random_premise = True
            p = make_tensor(np.random.choice(premises))
        
        if params.gpu_id > -1: 
            p, h, target = p.cuda(), h.cuda(). target.cuda()
        
        #print('h:', h, 'p:', p, 'target:', target)
        # make a batch of size 1
        #print('before unsqueeze:')
        #print(h); print(p); print(target)
        h, p, target = h.unsqueeze(0), p.unsqueeze(0), target.unsqueeze(0)
        #print('after unsqueeze:')
        #print(h); print(p); print(target)        

        k = h.size(0)  # actual batch size


        # model forward
        output_nli = nli_net(p, h, random_premise)   
        #print('output_nli:', output_nli)
        #print('output_nli.data:', output_nli.data)
        pred_nli = output_nli.data.max(1)[1]
        # no batch case
        #pred_nli = output_nli.argmax()
        #print('pred_nli:', pred_nli)
        # TODO: fix this to be non-batch 
        correct_nli += pred_nli.long().eq(target.data.long()).cpu().sum().item()
        #print('target:', target)
        #print('correct_nli:', correct_nli)
        #assert len(pred_nli) == len(hypoths[stidx:stidx + params.batch_size])
        
        if adv_lambda > 0:
            output_hypoth = hypoth_net(h)
            pred_hypoth = output_hypoth.data.max(1)[1]
            # no batch case
            #pred_hypoth = output_hypoth.argmax()
            # TODO: fix this to be non-batch
            correct_hypoth += pred_hypoth.long().eq(target.data.long()).cpu().sum().item()
            #assert len(pred_hypoth) == len(hypoths[stidx:stidx + params.batch_size])

        # loss
        #print('target:', target)
        loss_nli = loss_fn_nli(output_nli, target)
        all_costs_nli.append(loss_nli.item())
        if adv_lambda > 0:
            loss_hypoth = loss_fn_hypoth(output_hypoth, target)
            loss_hypoth *= adv_lambda
            all_costs_hypoth.append(loss_hypoth.item())
        #words_count += hypoths_batch.nelement() / params.word_emb_dim

        # backward
        optimizer_nli.zero_grad()
        if adv_lambda > 0:
            optimizer_hypoth.zero_grad()
        loss_nli.backward()
        if adv_lambda > 0:
            loss_hypoth.backward()


        def clip_gradients_and_step(net, k, optimizer):

            # gradient clipping (off by default)
            shrink_factor = 1
            total_norm = 0

            for p in net.parameters():
                if p.requires_grad:
                    p.grad.data.div_(k)    # divide by the actual batch size
                    total_norm += p.grad.data.norm() ** 2
            total_norm = np.sqrt(total_norm)

            if total_norm > params.max_norm:
                    shrink_factor = params.max_norm / total_norm
            current_lr = optimizer.param_groups[0]['lr'] # current lr (no external "lr", for adam)

            optimizer.param_groups[0]['lr'] = current_lr * shrink_factor # just for update
            
            # optimizer step
            optimizer.step()
            optimizer.param_groups[0]['lr'] = current_lr

        clip_gradients_and_step(nli_net, k, optimizer_nli)        
        if adv_lambda > 0:
            clip_gradients_and_step(hypoth_net, k, optimizer_hypoth)


        if len(all_costs_nli) == 100:
            logs.append('{0} ; loss nli {1} ; loss hypoth {2}'.format(
                                                        stidx, round(np.mean(all_costs_nli), 2),
                                                        round(np.mean(all_costs_hypoth), 2)))
            #print(logs[-1])
            words_count = 0
            #all_costs_nli, all_costs_hypoth = [], []

        '''
        #if params.verbose:
            #trained_sents += k
            #print "epoch: %d -- trained %d / %d sentences -- %f ms per sentence" % (epoch, trained_sents, len(hypoths),
                                                                                    1000 * (time.time() - start_time) / trained_sents) 
            #sys.stdout.flush()
        '''

    train_acc_nli = round(100. * correct_nli/len(hypoths), 2)
    train_acc_hypoth = round(100. * correct_hypoth/len(hypoths), 2)
    print('results : epoch {0} ; mean accuracy train nli : {1}, loss nli : {2} ; mean accuracy train hypoth : {3}, loss hypoth : {4}'
                    .format(epoch, train_acc_nli, round(np.mean(all_costs_nli), 2), train_acc_hypoth, round(np.mean(all_costs_hypoth), 2)))
    return train_acc_nli, train_acc_hypoth, nli_net, hypoth_net


def evaluate(epoch, valid, optimizer_nli, optimizer_hypoth, params, word_vec, nli_net, hypoth_net, eval_type='valid', final_eval=False, adv_lambda=1.):
    nli_net.eval()
    hypoth_net.eval()
    correct_nli, correct_hypoth = 0., 0.
    preds_nli, preds_hypoth = [], []
    global val_acc_best, lr, stop_training, adam_stop

    if eval_type == 'valid':
        print('\n{0} : Epoch {1}'.format(eval_type, epoch))

    hypoths = valid['hypoths'] #if eval_type == 'valid' else test['s1']
    premises = valid['premises'] #if eval_type == 'valid' else test['s2']
    targets = valid['lbls']

    for stidx in range(0, len(hypoths), params.batch_size):                
        h = make_tensor(hypoths[stidx]) 
        target = torch.tensor(targets[stidx], dtype=torch.long)
        p = make_tensor(premises[stidx])
        
        if params.gpu_id > -1: 
            p, h, target = p.cuda(), h.cuda(). target.cuda()        
            
        # make a batch of size 1
        h, p, target = h.unsqueeze(0), p.unsqueeze(0), target.unsqueeze(0)
        k = h.size(0)  # actual batch size            

        # model forward
        output_nli = nli_net(p, h)
        pred_nli = output_nli.data.max(1)[1]
        preds_nli.append(pred_nli.item())
        # no batch case
        #pred_nli = output_nli.argmax()
        # TODO: fix this to be non-batch 
        correct_nli += pred_nli.long().eq(target.data.long()).cpu().sum().item()
        if adv_lambda > 0:
            output_hypoth = hypoth_net(h)
            pred_hypoth = output_hypoth.data.max(1)[1]
            preds_hypoth.append(pred_hypoth.item())
            # no batch case
            #pred_hypoth = output_hypoth.argmax()
             # TODO: fix this to be non-batch 
            correct_hypoth += pred_hypoth.long().eq(target.data.long()).cpu().sum().item()

    # save model
    eval_acc_nli = round(100. * correct_nli / len(hypoths), 2)
    eval_acc_hypoth = round(100. * correct_hypoth / len(hypoths), 2)
    if final_eval:
        print('finalgrep : accuracy {0} : nli {1}, hypoth {2}'.format(eval_type, eval_acc_nli, eval_acc_hypoth))
    else:
        print('togrep : results : epoch {0} ; mean accuracy {1} : nli {2}, hypoth {3}'.format(epoch, eval_type, eval_acc_nli, eval_acc_hypoth))

    if eval_type == 'valid' and epoch <= params.n_epochs:
        if eval_acc_nli > val_acc_best:
            val_acc_best = eval_acc_nli
        else:
            if 'sgd' in params.optimizer:
                optimizer_nli.param_groups[0]['lr'] = optimizer_nli.param_groups[0]['lr'] / params.lrshrink
                print('Shrinking nli lr by : {0}. New lr = {1}'.format(params.lrshrink,
                                  optimizer_nli.param_groups[0]['lr']))
                optimizer_hypoth.param_groups[0]['lr'] = optimizer_hypoth.param_groups[0]['lr'] / params.lrshrink
                print('Shrinking hypoth lr by : {0}. New lr = {1}'.format(params.lrshrink,
                                  optimizer_hypoth.param_groups[0]['lr']))
                if optimizer_nli.param_groups[0]['lr'] < params.minlr:
                    stop_training = True
            if 'adam' in params.optimizer:
               # early stopping (at 2nd decrease in accuracy)
                stop_training = adam_stop
                adam_stop = True

    #print('premises:', premises)
    #print('hypoths:', hypoths)
    #print('targets:', targets)
    #print('preds_nli:', preds_nli)
    #print('preds_hypoth:', preds_hypoth)
    #if adv_lambda > 0:
    #    print(list(zip(premises, hypoths, targets, preds_nli, preds_hypoth)))
    #else:
    #    print(list(zip(premises, hypoths, targets, preds_nli)))

    
                    
    return eval_acc_nli, eval_acc_hypoth

def get_optimizer(s):
    """
    Parse optimizer parameters.
    Input should be of the form:
        - "sgd,lr=0.01"
        - "adagrad,lr=0.1,lr_decay=0.05"
    """
    if "," in s:
        method = s[:s.find(',')]
        optim_params = {}
        for x in s[s.find(',') + 1:].split(','):
            split = x.split('=')
            assert len(split) == 2
            assert re.match("^[+-]?(\d+(\.\d*)?|\.\d+)$", split[1]) is not None
            optim_params[split[0]] = float(split[1])
    else:
        method = s
        optim_params = {}

    if method == 'adadelta':
        optim_fn = optim.Adadelta
    elif method == 'adagrad':
        optim_fn = optim.Adagrad
    elif method == 'adam':
        optim_fn = optim.Adam
    elif method == 'adamax':
        optim_fn = optim.Adamax
    elif method == 'asgd':
        optim_fn = optim.ASGD
    elif method == 'rmsprop':
        optim_fn = optim.RMSprop
    elif method == 'rprop':
        optim_fn = optim.Rprop
    elif method == 'sgd':
        optim_fn = optim.SGD
        assert 'lr' in optim_params
    else:
        raise Exception('Unknown optimization method: "%s"' % method)

    # check that we give good parameters to the optimizer
    expected_args = inspect.getargspec(optim_fn.__init__)[0]
    assert expected_args[:2] == ['self', 'params']
    if not all(k in expected_args[2:] for k in optim_params.keys()):
        raise Exception('Unexpected parameters: expected "%s", got "%s"' % (
            str(expected_args[2:]), str(optim_params.keys())))

    return optim_fn, optim_params



def get_args():
    parser = argparse.ArgumentParser(description='Synthetic experiment')

    # training
    parser.add_argument("--n_epochs", type=int, default=20)
    parser.add_argument("--batch_size", type=int, default=1)
    parser.add_argument("--optimizer", type=str, default="sgd,lr=0.1", help="adam or sgd,lr=0.1")
    #parser.add_argument("--optimizer", type=str, default="adam", help="adam or sgd,lr=0.1")
    parser.add_argument("--lrshrink", type=float, default=5, help="shrink factor for sgd")
    parser.add_argument("--decay", type=float, default=0.99, help="lr decay")
    parser.add_argument("--minlr", type=float, default=1e-5, help="minimum lr")
    parser.add_argument("--max_norm", type=float, default=5., help="max norm (grad clipping)")
    parser.add_argument("--adv_lambda", type=float, default=0.0, help="adversarial loss weight for hyp only classifier")
    parser.add_argument("--adv_hyp_encoder_lambda", type=float, default=0.0, help="adversarial weight for hypothesis only encoder")
    parser.add_argument("--nli_net_adv_hyp_encoder_lambda", type=float, default=0.0, help="adversarial weight for hypothesis encoder in NLI net")
    parser.add_argument("--random_premise_frac", type=float, default=0.0, help="fraction of random premises to use in NLI net")

    # model
    parser.add_argument("--embedding_dim", type=int, default=10, help="embedding dimensionality")
    parser.add_argument("--num_classes", type=int, default=2, help="number of classes")
    parser.add_argument("--dictionary_size", type=int, default=29, help="dictionary size")
    parser.add_argument("--hidden_dim", type=int, default=10, help="hidden dimensionality")
    parser.add_argument("--directions", type=int, default=1, help="directions for LSTM")

    # gpu
    parser.add_argument("--gpu_id", type=int, default=-1, help="GPU ID")


    params, _ = parser.parse_known_args()
    params.dictionary_size = len(word_to_idx)

    # print parameters passed, and all parameters
    print(sys.argv[1:])
    print(params)

    return params




def main(args):

    
    # TODO: dummy, get rid of this
    word_vecs = None
    
    
    """ Single symbol case """
    """
    # TODDO: move this somewhere
    hypoth_length = 1
    op = 'cat'
    
    # training set is biased
    train_ph2frac = {('a', 'a'): 0.4, ('a', 'b'): 0.4, ('b', 'a'): 0.1, ('b', 'b'): 0.1}
    #train_ph2frac = {('a', 'a'): 0.49, ('a', 'b'): 0.49, ('b', 'a'): 0.01, ('b', 'b'): 0.01}
    train_total = 1000
    # test set is unbiased
    test_ph2frac = {('a', 'a'): 0.25, ('a', 'b'): 0.25, ('b', 'a'): 0.25, ('b', 'b'): 0.25}
    test_total = 1000
    # true frac is the same for train and test (it is just used for ambiguity)'    
    ph2truefrac = {('a', 'a'): 0.8, ('a', 'b'): 0.2, ('b', 'a'): 0.2, ('b', 'b'): 0.8}
    
    ph2truefrac_unambig = {('a', 'a'): 1.0, ('a', 'b'): 0.0, ('b', 'a'): 0.0, ('b', 'b'): 1.0}
    """
    
    
    """ Insert biased symbol into hypoth, use 'c' and 'p' and op=cat """
    """
    hypoth_length = 2
    op = 'cat'
    
    # training set is biased
    #train_ph2frac = {('a', 'ac'): 0.4, ('a', 'bp'): 0.4, ('b', 'ap'): 0.1, ('b', 'bc'): 0.1}
    train_ph2frac = {('a', 'ac'): 0.25, ('a', 'bp'): 0.25, ('b', 'ap'): 0.25, ('b', 'bc'): 0.25}
    train_total = 1000
    # test set is unbiased
    test_ph2frac = {('a', 'ap'): 0.25, ('a', 'bp'): 0.25, ('b', 'ap'): 0.25, ('b', 'bp'): 0.25}
    test_total = 1000
    # true frac is the same for train and test (it is just used for ambiguity)'    
    ph2truefrac = {('a', 'ac'): 0.8, ('a', 'bp'): 0.2, ('b', 'ap'): 0.2, ('b', 'bc'): 0.8, 
                   ('a', 'ap'): 0.8, ('b', 'bp'): 0.8}
    
    ph2truefrac_unambig = {('a', 'ac'): 1.0, ('a', 'bp'): 0.0, ('b', 'ap'): 0.0, ('b', 'bc'): 1.0, 
                   ('a', 'ap'): 1.0, ('b', 'bp'): 1.0}
    """


    """ Insert biased symbol into hypoth, use only 'c' and op=sum """
    #"""
    hypoth_length = 2
    op = 'sum'
    
    # training set is biased
    #train_ph2frac = {('a', 'ac'): 0.4, ('a', 'b'): 0.4, ('b', 'a'): 0.1, ('b', 'bc'): 0.1}
    #train_ph2frac = {('a', 'ac'): 0.25, ('a', 'b'): 0.25, ('b', 'a'): 0.25, ('b', 'bc'): 0.25}
    #train_total = 1000
    # test set is unbiased
    #test_ph2frac = {('a', 'a'): 0.25, ('a', 'b'): 0.25, ('b', 'a'): 0.25, ('b', 'b'): 0.25}
    #test_total = 1000
    # true frac is the same for train and test (it is just used for ambiguity)'    
    #ph2truefrac = {('a', 'ac'): 0.8, ('a', 'b'): 0.2, ('b', 'a'): 0.2, ('b', 'bc'): 0.8, 
    #               ('a', 'a'): 0.8, ('b', 'b'): 0.8}
    
    #ph2truefrac_unambig = {('a', 'ac'): 1.0, ('a', 'b'): 0.0, ('b', 'a'): 0.0, ('b', 'bc'): 1.0, 
    #               ('a', 'a'): 1.0, ('b', 'b'): 1.0}   
    #"""

    
    
    """ Insert biased symbol into hypoth, use only 'c' and op=sum, fraction of 'c' """
    """
    hypoth_length = 2
    op = 'sum'
    
    # training set is biased
    #train_ph2frac = {('a', 'ac'): 0.4, ('a', 'b'): 0.4, ('b', 'a'): 0.1, ('b', 'bc'): 0.1}
    train_ph2frac = {('a', 'ac'): 0.249, ('a', 'a'): 0.001, ('a', 'b'): 0.25, ('b', 'a'): 0.25, ('b', 'bc'): 0.249, ('b', 'b'):0.001}
    train_total = 1000
    # test set is unbiased
    test_ph2frac = {('a', 'a'): 0.25, ('a', 'b'): 0.25, ('b', 'a'): 0.25, ('b', 'b'): 0.25}
    test_total = 1000
    # true frac is the same for train and test (it is just used for ambiguity)'    
    ph2truefrac = {('a', 'ac'): 0.8, ('a', 'b'): 0.2, ('b', 'a'): 0.2, ('b', 'bc'): 0.8, 
                   ('a', 'a'): 0.8, ('b', 'b'): 0.8}
    
    ph2truefrac_unambig = {('a', 'ac'): 1.0, ('a', 'b'): 0.0, ('b', 'a'): 0.0, ('b', 'bc'): 1.0, 
                   ('a', 'a'): 1.0, ('b', 'b'): 1.0}        
    """
    

    #train = make_data(train_ph2frac, ph2truefrac, train_total)
    #test = make_data(test_ph2frac, ph2truefrac, test_total)
    train = make_data('train.txt_data')
    test = make_data('data_test_exam')


    # define premise and hypoth encoders
    #premise_encoder = Encoder(nn.Embedding(num_embeddings=args.dictionary_size, embedding_dim=args.embedding_dim)) #, padding_idx=0))
    #hypoth_encoder = Encoder(nn.Embedding(num_embeddings=args.dictionary_size, embedding_dim=args.embedding_dim)) #, padding_idx=0))
    premise_encoder = Encoder(nn.Embedding(num_embeddings=args.dictionary_size, embedding_dim=args.embedding_dim), embedding_dim=args.embedding_dim, hidden_dim=args.hidden_dim, directions=args.directions) #, padding_idx=0))
    hypoth_encoder = Encoder(nn.Embedding(num_embeddings=args.dictionary_size, embedding_dim=args.embedding_dim), embedding_dim=args.embedding_dim, hidden_dim=args.hidden_dim, directions=args.directions) #, padding_idx=0))

    nli_net = NLINet(args.embedding_dim, args.num_classes, args.nli_net_adv_hyp_encoder_lambda, premise_encoder, hypoth_encoder, hypoth_length=hypoth_length, op=op)
    #nli_net = NLINet(args.embedding_dim, args.num_classes, args.nli_net_adv_hyp_encoder_lambda, hypoth_encoder, hypoth_encoder)
    hypoth_net = HypothNet(args.embedding_dim, args.num_classes, args.adv_hyp_encoder_lambda, hypoth_encoder, hypoth_length=hypoth_length, op=op)
    print(nli_net)
    print(hypoth_net)

    # nli loss
    weight = torch.FloatTensor(args.num_classes).fill_(1)
    loss_fn_nli = nn.CrossEntropyLoss(weight=weight)
    loss_fn_nli.size_average = False

    # hypoth (adversarial) loss
    weight = torch.FloatTensor(args.num_classes).fill_(1)
    loss_fn_hypoth = nn.CrossEntropyLoss(weight=weight)
    loss_fn_hypoth.size_average = False

    # optimizer
    optim_fn, optim_params = get_optimizer(args.optimizer)
    #optim_fn = optim.SGD
    #optim_params = {'lr': 0.01}
    optimizer_nli = optim_fn(nli_net.parameters(), **optim_params)
    optimizer_hypoth = optim_fn(hypoth_net.classifier.parameters(), **optim_params)

    if args.gpu_id > -1:
        nli_net.cuda()
        hypoth_net.cuda()
        loss_fn_nli.cuda()
        loss_fn_hypoth.cuda()

    """
    TRAIN
    """
    global val_acc_best, lr, stop_training, adam_stop
    val_acc_best = -1e10
    adam_stop = False
    stop_training = False
    lr = optim_params['lr'] if 'sgd' in args.optimizer else None

    """
    Train model on Natural Language Inference task
    """
    epoch = 1

    while not stop_training and epoch <= args.n_epochs:
        train_acc_nli, train_acc_hypoth, nli_net, hypoth_net = trainepoch(epoch, train, optimizer_nli, optimizer_hypoth, args, word_vecs, nli_net, hypoth_net, loss_fn_nli, loss_fn_hypoth, args.adv_lambda, args.adv_hyp_encoder_lambda)
        #eval_acc_nli, eval_acc_hypoth = evaluate(epoch, val, optimizer_nli, optimizer_hypoth, args, word_vecs, nli_net, hypoth_net, 'valid', adv_lambda=args.adv_lambda)
        eval_acc_nli_train, eval_acc_hypoth_train = evaluate(epoch, train, optimizer_nli, optimizer_hypoth, args, word_vecs, nli_net, hypoth_net, 'train', adv_lambda=args.adv_lambda)
        eval_acc_nli, eval_acc_hypoth = evaluate(epoch, test, optimizer_nli, optimizer_hypoth, args, word_vecs, nli_net, hypoth_net, 'valid', adv_lambda=args.adv_lambda)
        epoch += 1

    return eval_acc_nli_train, eval_acc_hypoth_train, eval_acc_nli, eval_acc_hypoth


if __name__ == '__main__':
    args = get_args()
    if args.gpu_id > -1:
        torch.cuda.manual_seed(seed)
    eval_acc_nli_train, eval_acc_hypoth_train, eval_acc_nli, eval_acc_hypoth = main(args)
    #print(eval_acc_nli_train, eval_acc_hypoth_train, eval_acc_nli, eval_acc_hypoth)